{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d78772e-cee3-45c2-ac4f-51cf66a69a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: -0.5080175709423214\n",
      "MSE: 921557814.7863477\n",
      "Cross-validated R² scores: [-0.48910974 -0.11530741 -0.3261574  -0.39049531 -0.12709978]\n",
      "Average R²: -0.28963392957926126\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100),\n",
    "    'Target': np.random.uniform(50000, 150000, 100)  # Regression target\n",
    "})\n",
    "\n",
    "# Add some missing values and outliers\n",
    "df.loc[5:10, 'Salary'] = np.nan\n",
    "df.loc[3, 'Age'] = 150  # outlier\n",
    "\n",
    "# Step 2: Define feature groups\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Create preprocessors\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),        # Handle missing\n",
    "    (\"scaler\", StandardScaler())                          # Scale\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))    # Encode\n",
    "])\n",
    "\n",
    "# Step 4: Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 5: Build full pipeline with model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),  # You can set k=5 for top 5\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Step 6: Split data\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Optional: Cross-validation\n",
    "cv_scores = cross_val_score(model_pipeline, X, y, cv=5, scoring='r2')\n",
    "print(\"Cross-validated R² scores:\", cv_scores)\n",
    "print(\"Average R²:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ead2dc-e072-4b00-9f56-92283484b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: -0.6039556737677059\n",
      "MSE: 980186116.0065136\n",
      "Cross-validated R² scores: [-0.09356489 -0.35854617 -0.327535   -0.13176058 -0.18850697]\n",
      "Average R²: -0.21998272010979747\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100),\n",
    "    'Target': np.random.uniform(50000, 150000, 100)  # Regression target\n",
    "})\n",
    "\n",
    "# Add some missing values and outliers\n",
    "df.loc[5:10, 'Salary'] = np.nan\n",
    "df.loc[3, 'Age'] = 150  # outlier\n",
    "\n",
    "# Step 2: Define feature groups\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Create preprocessors\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),        # Handle missing\n",
    "    (\"scaler\", StandardScaler())                          # Scale\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))    # Encode\n",
    "])\n",
    "\n",
    "# Step 4: Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 5: Build full pipeline with model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),  # You can set k=5 for top 5\n",
    "    (\"regressor\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Step 6: Split data\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Optional: Cross-validation\n",
    "cv_scores = cross_val_score(model_pipeline, X, y, cv=5, scoring='r2')\n",
    "print(\"Cross-validated R² scores:\", cv_scores)\n",
    "print(\"Average R²:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedff965-f424-475f-ae7d-471767213e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fffdea2-5051-44f5-a64f-14816217780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: -1.464935256431548\n",
      "MSE: 1506335464.7037907\n",
      "Cross-validated R² scores: [-0.27920506 -1.02019806 -0.71152508 -0.22242395 -0.10840132]\n",
      "Average R²: -0.4683506942119656\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100),\n",
    "    'Target': np.random.uniform(50000, 150000, 100)  # Regression target\n",
    "})\n",
    "\n",
    "# Add some missing values and outliers\n",
    "df.loc[5:10, 'Salary'] = np.nan\n",
    "df.loc[3, 'Age'] = 150  # outlier\n",
    "\n",
    "# Step 2: Define feature groups\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Create preprocessors\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),        # Handle missing\n",
    "    (\"scaler\", StandardScaler())                          # Scale\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))    # Encode\n",
    "])\n",
    "\n",
    "# Step 4: Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 5: Build full pipeline with model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),  # You can set k=5 for top 5\n",
    "    (\"regressor\", XGBRegressor())\n",
    "])\n",
    "\n",
    "# Step 6: Split data\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Optional: Cross-validation\n",
    "cv_scores = cross_val_score(model_pipeline, X, y, cv=5, scoring='r2')\n",
    "print(\"Cross-validated R² scores:\", cv_scores)\n",
    "print(\"Average R²:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73329457-6611-4d68-b03c-36b711320c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 5, 'regressor__n_estimators': 50, 'regressor__subsample': 0.8}\n",
      "R² Score: -0.3114067037373993\n",
      "MSE: 801407834.682736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100),\n",
    "    'Target': np.random.uniform(50000, 150000, 100)\n",
    "})\n",
    "\n",
    "# Add missing values and outliers\n",
    "df.loc[5:10, 'Salary'] = np.nan\n",
    "df.loc[3, 'Age'] = 150  # outlier\n",
    "\n",
    "# Step 2: Define features\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 4: Full pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Step 5: Parameter Grid for XGBoost\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100],\n",
    "    'regressor__max_depth': [3, 5],\n",
    "    'regressor__learning_rate': [0.01, 0.1],\n",
    "    'regressor__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Step 6: GridSearchCV\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3def289-f208-43f0-80c5-924e45e54a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__subsample': 0.8, 'regressor__n_estimators': 50, 'regressor__max_depth': 5, 'regressor__learning_rate': 0.01}\n",
      "R² Score: -0.3114067037373993\n",
      "MSE: 801407834.682736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100),\n",
    "    'Target': np.random.uniform(50000, 150000, 100)\n",
    "})\n",
    "\n",
    "# Add missing values and outliers\n",
    "df.loc[5:10, 'Salary'] = np.nan\n",
    "df.loc[3, 'Age'] = 150  # outlier\n",
    "\n",
    "# Step 2: Define features\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 4: Full pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Step 5: Parameter Grid for XGBoost\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100],\n",
    "    'regressor__max_depth': [3, 5],\n",
    "    'regressor__learning_rate': [0.01, 0.1],\n",
    "    'regressor__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Step 6: GridSearchCV\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Random_search = RandomizedSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)  # Model \n",
    "Random_search.fit(X_train, y_train) # fit\n",
    "\n",
    "# Step 7: Evaluation\n",
    "best_model = Random_search.best_estimator_ # Additional\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", Random_search.best_params_)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303e2abb-cb55-4ed0-8937-6c8ccda79daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__subsample': 0.8, 'regressor__n_estimators': 100, 'regressor__max_depth': 3, 'regressor__learning_rate': 0.1}\n",
      "R² Score: 0.8172038433670109\n",
      "MSE: 87575446.31760874\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Step 1: Simulate a realistic dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Salary': np.random.randint(30000, 120000, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100),\n",
    "    'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),\n",
    "    'City': np.random.choice(['NY', 'LA', 'SF', 'Chicago'], 100)\n",
    "})\n",
    "\n",
    "# Create a logical target\n",
    "df['Target'] = (\n",
    "    df['Salary'] * 0.6 +\n",
    "    df['Experience'] * 2000 +\n",
    "    df['Age'] * 100 +\n",
    "    np.random.normal(0, 5000, 100)  # Add some noise\n",
    ")\n",
    "# Step 2: Define features\n",
    "numerical_features = ['Age', 'Salary', 'Experience']\n",
    "categorical_features = ['Education', 'City']\n",
    "\n",
    "# Step 3: Pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_features),\n",
    "    (\"cat\", categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 4: Full pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_regression, k='all')),\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Step 5: Parameter Grid for XGBoost\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100],\n",
    "    'regressor__max_depth': [3, 5],\n",
    "    'regressor__learning_rate': [0.01, 0.1],\n",
    "    'regressor__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Step 6: GridSearchCV\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Random_search = RandomizedSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "Random_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluation\n",
    "best_model = Random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", Random_search.best_params_)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ce94f-9c80-4d9f-bed8-fd53e4771bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
