{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ffca4d-0788-4e54-adce-d20e17da4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n",
      "Found existing installation: tensorflow-intel 2.16.1\n",
      "Uninstalling tensorflow-intel-2.16.1:\n",
      "  Successfully uninstalled tensorflow-intel-2.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow-gpu as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow tensorflow-intel tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9a8b38-d926-4695-bc44-1f84b874438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Word EMBEDDINGS\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load English model with word vectors\n",
    "nlp = spacy.load('en_core_web_md')  # Make sure to install this: python -m spacy download en_core_web_md\n",
    "\n",
    "# Example dataset: text and sentiment\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Stocks are up today\",\n",
    "        \"The market crashed badly\",\n",
    "        \"Company shows strong earnings\",\n",
    "        \"Investors fear inflation\",\n",
    "        \"Shares fell due to bad news\",\n",
    "        \"Profits increased significantly\",\n",
    "        \"Negative outlook on economy\",\n",
    "        \"Bullish momentum in the market\",\n",
    "        \"Company reports record revenue\",\n",
    "        \"Economic uncertainty looms\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]  # 1 = Positive sentiment, 0 = Negative sentiment\n",
    "})\n",
    "\n",
    "# Convert each text to an average word embedding\n",
    "def get_vector(text):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector  # 300-dimensional average vector\n",
    "\n",
    "# Apply to dataset\n",
    "data['vector'] = data['text'].apply(get_vector)\n",
    "\n",
    "# Train/test split\n",
    "X = list(data['vector'])\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train simple classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84365e8a-d21e-43e3-a4e8-69988f86d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7619644283461814\n"
     ]
    }
   ],
   "source": [
    "#Example: Word Embedding Usage\n",
    "\n",
    "doc1 = nlp(\"The stock is rising\")\n",
    "doc2 = nlp(\"The share price went up\")\n",
    "\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)  # Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ba36b9-6a57-4587-ab21-4fc577f2f2d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m---> 33\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Step 6: Evaluate performance\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1335\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1333\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1339\u001b[0m     )\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1342\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "#Workflow Summary Using spaCy Word Embeddings + Machine Learning\n",
    "# Step 1: Load the word embedding model\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "#Step 2: Convert each text into a vector\n",
    "#For a list of texts like news headlines or articles:\n",
    "\n",
    "texts = [\"Nvidia shares jump after earnings\", \n",
    "         \"Stock market crashes due to global unrest\"]\n",
    "\n",
    "X = [nlp(text).vector for text in texts]\n",
    "#This gives you a dense vector (300 dimensions) for each sentence.\n",
    "\n",
    "#Each vector is a numerical representation (embedding) of the sentence.\n",
    "\n",
    "# Step 3: Assign labels for training\n",
    "#For binary classification (e.g., positive: 1, negative: 0):\n",
    "\n",
    "y = [1, 0]  # Labels for sentiment\n",
    "#ðŸ”¹ Step 4: Train/test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#ðŸ”¹ Step 5: Train the ML model\n",
    "#Use Logistic Regression for binary sentiment classification:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Step 6: Evaluate performance\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "733ff673-b1d4-4941-96bc-978033e7996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "#Steps:Load spaCy with pre-trained word vectors\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "#Prepare sample news texts and labels\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"Nvidia stock surges after strong earnings report\",\n",
    "    \"Apple shares drop amid supply chain issues\",\n",
    "    \"Tesla's new battery tech excites investors\",\n",
    "    \"Stock market tumbles due to inflation concerns\",\n",
    "    \"Microsoft sees record profits in Q2\",\n",
    "    \"Amazon hit with major antitrust lawsuit\",\n",
    "    \"Google announces layoffs in AI division\",\n",
    "    \"Meta launches new social platform\",\n",
    "    \"Intel struggles with chip delays\",\n",
    "    \"Netflix gains 10 million new subscribers\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1 = Positive news, 0 = Negative news\n",
    "#Convert each text to a vector\n",
    "\n",
    "\n",
    "X = [nlp(text).vector for text in texts]\n",
    "y = labels\n",
    "#Split into train/test sets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y  # \n",
    ")\n",
    "#Train Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "#Evaluate the model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe7a72-6aea-440c-bfe4-5c150d5be448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Code does not work \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain 'text' and 'label' columns: {file_path}\")\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Remove empty or invalid rows\n",
    "    df = df[(df['text'].str.len() > 0) & (df['label'].isin(label_map.keys()))]\n",
    "\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in file {file_path} after cleaning!\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "train_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "def tokenize_data(texts):\n",
    "    if not texts or len(texts) == 0:\n",
    "        raise ValueError(\"Empty text list passed to tokenizer!\")\n",
    "\n",
    "    # Filter again just in case (remove empty or non-string)\n",
    "    texts = [str(t) for t in texts if isinstance(t, str) and len(t) > 0]\n",
    "\n",
    "    if len(texts) == 0:\n",
    "        raise ValueError(\"No valid text entries found after filtering!\")\n",
    "\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=label_map.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19ef015-14bd-4fab-bef9-975151dc8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say X is the combined data from nvidia.csv and stock.csv\n",
    "# X shape: (sequence_length, number_of_stocks)\n",
    "\n",
    "sequence_length = 10  # last 10 days\n",
    "number_of_stocks = 5  # Nvidia + 4 others\n",
    "\n",
    "# Initialize some random attention weights (in real transformers these are learned)\n",
    "attention_weights = np.random.rand(sequence_length, sequence_length)  # for simplicity\n",
    "\n",
    "# Normalize weights so rows sum to 1 (softmax)\n",
    "attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Apply attention: for each day, summarize info from all days weighted by attention\n",
    "X_attention = np.dot(attention_weights, X)  # shape: (sequence_length, number_of_stocks)\n",
    "\n",
    "# Now X_attention has combined info with attention applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b1cbc2-d613-4b82-8a51-afa29203db12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid data found in file C:/Users/senth/OneDrive/Desktop/nvidia.csv after cleaning!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Load train and test data\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m train_df \u001b[38;5;241m=\u001b[39m load_and_prepare_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/senth/OneDrive/Desktop/nvidia.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m test_df \u001b[38;5;241m=\u001b[39m load_and_prepare_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/senth/OneDrive/Desktop/stock.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_data\u001b[39m(texts):\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36mload_and_prepare_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(label_map)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid data found in file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after cleaning!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: No valid data found in file C:/Users/senth/OneDrive/Desktop/nvidia.csv after cleaning!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Map text labels to integers\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    # Check required columns\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain 'text' and 'label' columns: {file_path}\")\n",
    "\n",
    "    # Clean text and labels\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Filter valid rows\n",
    "    df = df[(df['text'].str.len() > 0) & (df['label'].isin(label_map.keys()))]\n",
    "\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in file {file_path} after cleaning!\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load train and test data\n",
    "train_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "def tokenize_data(texts):\n",
    "    texts = [str(t) for t in texts if isinstance(t, str) and len(t) > 0]\n",
    "\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize texts\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "\n",
    "# Custom Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "# Load model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Predict on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=list(label_map.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2e32a7-7c51-480e-b107-b7ff0f059cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 139: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load your data files\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train_df \u001b[38;5;241m=\u001b[39m load_and_prepare_data_numeric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/senth/OneDrive/Desktop/nvidia.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m test_df \u001b[38;5;241m=\u001b[39m load_and_prepare_data_numeric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/senth/OneDrive/Desktop/stock.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mload_and_prepare_data_numeric\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_prepare_data_numeric\u001b[39m(file_path):\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 139: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data_numeric(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain 'text' and 'label' columns: {file_path}\")\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "\n",
    "    # Check labels are numeric and only 0 or 1\n",
    "    valid_labels = [0, 1]\n",
    "    df = df[df['label'].isin(valid_labels)]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in file {file_path} after filtering!\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load your data files\n",
    "train_df = load_and_prepare_data_numeric(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data_numeric(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e998e5-15c6-4dee-971f-5416b1a22ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read C:/Users/senth/OneDrive/Desktop/nvidia.csv with encoding utf-8, trying next...\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/nvidia.csv with encoding latin1\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/stock.csv with encoding utf-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'io'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m\n\u001b[0;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\n\u001b[0;32m     78\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     79\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     85\u001b[0m )\n\u001b[1;32m---> 86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     90\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     93\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     95\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:682\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    680\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[0;32m    681\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m CallbackHandler(\n\u001b[0;32m    683\u001b[0m     callbacks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:449\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, processing_class, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(cb)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;241m=\u001b[39m processing_class\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:466\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[1;32m--> 466\u001b[0m     cb \u001b[38;5;241m=\u001b[39m callback() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[0;32m    467\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:665\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[1;34m(self, tb_writer)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_SummaryWriter \u001b[38;5;241m=\u001b[39m SummaryWriter\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecordWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaryWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(load_once(\u001b[38;5;28mself\u001b[39m), attr_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Label names and number of classes\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "num_labels = len(label_names)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def safe_read_csv(file_path):\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"Successfully read {file_path} with encoding {enc}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read {file_path} with encoding {enc}, trying next...\")\n",
    "    raise UnicodeDecodeError(f\"Could not read {file_path} with tried encodings.\")\n",
    "\n",
    "def load_and_prepare_data_numeric(file_path):\n",
    "    df = safe_read_csv(file_path)\n",
    "\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain 'text' and 'label' columns: {file_path}\")\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "\n",
    "    # Only keep rows with label 0 or 1\n",
    "    valid_labels = [0, 1]\n",
    "    df = df[df['label'].isin(valid_labels)]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in file {file_path} after filtering!\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load datasets\n",
    "train_df = load_and_prepare_data_numeric(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data_numeric(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "def tokenize_data(texts):\n",
    "    texts = [str(t) for t in texts if isinstance(t, str) and len(t) > 0]\n",
    "\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4713c332-b692-457b-b9e7-7dc8a4069fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\senth\\anaconda3\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\senth\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022fee97-10c3-445f-801c-4328c4a3bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2a41bc-1717-488d-8bf0-83f6f640bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping tensorflow-gpu as it is not installed.\n",
      "WARNING: Skipping tensorflow-estimator as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow tensorflow-gpu tensorflow-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abeb47f-41d2-42b8-b7b7-54b0f6bd782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensorflow\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensorflow\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__file__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0906f25-9e4a-4a6a-b55a-3bae8ca19d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7bbd24-d8bd-4c7d-a8f1-de2a57155302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bac0b69-ffdf-4490-b694-5918430ff4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed with encoding utf-8\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/nvidia.csv with encoding latin1\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/stock.csv with encoding utf-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'io'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 99\u001b[0m\n\u001b[0;32m     88\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     89\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m   \n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    100\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    101\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m    102\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m    103\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    107\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:682\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    680\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[0;32m    681\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m CallbackHandler(\n\u001b[0;32m    683\u001b[0m     callbacks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:449\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, processing_class, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(cb)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;241m=\u001b[39m processing_class\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:466\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[1;32m--> 466\u001b[0m     cb \u001b[38;5;241m=\u001b[39m callback() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[0;32m    467\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:665\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[1;34m(self, tb_writer)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_SummaryWriter \u001b[38;5;241m=\u001b[39m SummaryWriter\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecordWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaryWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(load_once(\u001b[38;5;28mself\u001b[39m), attr_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Set label names and number of classes\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Utility function to safely load CSVs\n",
    "def safe_read_csv(file_path):\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"Successfully read {file_path} with encoding {enc}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed with encoding {enc}\")\n",
    "    raise UnicodeDecodeError(\"All encoding attempts failed\")\n",
    "\n",
    "# Load and clean dataset\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = safe_read_csv(file_path)\n",
    "\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'text' and 'label' columns\")\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df = df[df['label'].isin([0, 1])]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in {file_path}\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load your datasets\n",
    "train_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "\n",
    "# Custom Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    "  \n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58586a94-1648-4601-ae03-479b0984ab5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'io'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.io.gfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee3f95c-61ad-46af-bfca-342f430a32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfac124e-1c18-4720-9f0d-6a461576bcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed with encoding utf-8\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/nvidia.csv with encoding latin1\n",
      "Successfully read C:/Users/senth/OneDrive/Desktop/stock.csv with encoding utf-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'io'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 99\u001b[0m\n\u001b[0;32m     88\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     89\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    100\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    101\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m    102\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m    103\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    107\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:682\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    680\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[0;32m    681\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m CallbackHandler(\n\u001b[0;32m    683\u001b[0m     callbacks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:449\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, processing_class, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(cb)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;241m=\u001b[39m processing_class\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:466\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[1;32m--> 466\u001b[0m     cb \u001b[38;5;241m=\u001b[39m callback() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[0;32m    467\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:665\u001b[0m, in \u001b[0;36mTensorBoardCallback.__init__\u001b[1;34m(self, tb_writer)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_SummaryWriter \u001b[38;5;241m=\u001b[39m SummaryWriter\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecordWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaryWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(load_once(\u001b[38;5;28mself\u001b[39m), attr_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Set label names and number of classes\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Utility function to safely load CSVs\n",
    "def safe_read_csv(file_path):\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"Successfully read {file_path} with encoding {enc}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed with encoding {enc}\")\n",
    "    raise UnicodeDecodeError(\"All encoding attempts failed\")\n",
    "\n",
    "# Load and clean dataset\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = safe_read_csv(file_path)\n",
    "\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'text' and 'label' columns\")\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df = df[df['label'].isin([0, 1])]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid data found in {file_path}\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Load your datasets\n",
    "train_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "\n",
    "# Custom Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    "    \n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49332ac6-e4eb-447e-9498-629b9ed5ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file C:/Users/senth/OneDrive/Desktop/nvidia.csv using encoding latin1\n",
      "Read file C:/Users/senth/OneDrive/Desktop/stock.csv using encoding utf-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67         2\n",
      "    positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.25      0.50      0.33         4\n",
      "weighted avg       0.25      0.50      0.33         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- Constants ---\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "num_labels = len(label_names)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# --- File Reading with Encoding Handling ---\n",
    "def safe_read_csv(file_path):\n",
    "    for enc in ['utf-8', 'latin1', 'ISO-8859-1']:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f\"Read file {file_path} using encoding {enc}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(f\"Could not read file: {file_path} with common encodings.\")\n",
    "\n",
    "# --- Load and Clean Data ---\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = safe_read_csv(file_path)\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"CSV must have 'text' and 'label' columns\")\n",
    "    \n",
    "    df = df[df['label'].isin([0, 1])]\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "train_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/nvidia.csv\")\n",
    "test_df = load_and_prepare_data(\"C:/Users/senth/OneDrive/Desktop/stock.csv\")\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_df['text'].tolist())\n",
    "test_encodings = tokenize_texts(test_df['text'].tolist())\n",
    "\n",
    "# --- Dataset Wrapper ---\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "# --- Model ---\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=None,  # Disable TensorBoard\n",
    "    report_to=[],      # Prevent default logging to TensorBoard\n",
    ")\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# --- Training ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Prediction ---\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], preds, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d100f5-c0a2-4634-a346-a50261c87290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Prompt ===\n",
      "The future of AI is unknown.\"\n",
      "\n",
      "That's not to say that AI is just going to be a reality. Many people are already aware that AI is a lot like Facebook, and that's why it's so important to get involved.\n",
      "\n",
      "While some companies like to invest in research and development to bring AI to market, many others have had to invest heavily in development and build on the work of other AI companies. Some of these companies have been successful at trying to make AI a reality\n",
      "\n",
      "=== Engineered Prompt ===\n",
      "You are an expert AI researcher. Explain in simple terms: What is the future of artificial intelligence? Do you have any predictions for the future of AI, and what do you think about the future of AI?\n",
      "\n",
      "I can't answer that question, because my answer is that a lot of things are going to change in the next couple of decades. There's going to be an explosion of new systems, and there will be a bunch of new AI technologies that will be used for the most part\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model (small GPT-2)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to generate text based on prompt\n",
    "def generate_text(prompt, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,           # Enable sampling to get creative outputs\n",
    "        top_k=50,                 # Limits token selection to top k tokens\n",
    "        top_p=0.95,               # Nucleus sampling\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return generated texts\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Basic prompt\n",
    "prompt_basic = \"The future of AI is\"\n",
    "\n",
    "# Engineered prompt (more context)\n",
    "prompt_engineered = (\n",
    "    \"You are an expert AI researcher. Explain in simple terms: \"\n",
    "    \"What is the future of artificial intelligence?\"\n",
    ")\n",
    "\n",
    "print(\"=== Basic Prompt ===\")\n",
    "print(generate_text(prompt_basic)[0])\n",
    "\n",
    "print(\"\\n=== Engineered Prompt ===\")\n",
    "print(generate_text(prompt_engineered)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed1c24e2-2243-4d31-8dd7-467b46406555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "1: Retrieval augmented generation combines search with language generation.\n",
      "2: Transformers are models used widely in NLP tasks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b85343fa534198ace8380e5718c88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senth\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\senth\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74e6fb223b34d9e8aefff047faba911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540242f3dd5f41a6b825e37469edc682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe30f2cbfa4f18be81570a31fcf61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da7fb904ae54a56b95f2f1b17c7d207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeef053445034016ade761a4617227c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " Retrieval augmented generation combines search with language generation. Transformers are models used widely in NLP tasks. Question: What is retrieval augmented generation? Back to Mail Online home. Back to the page you came from. Click here for more information on retrieving augmented generation.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: Prepare corpus (documents to retrieve from)\n",
    "corpus = [\n",
    "    \"Artificial intelligence is a branch of computer science.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Transformers are models used widely in NLP tasks.\",\n",
    "    \"Retrieval augmented generation combines search with language generation.\",\n",
    "    \"FAISS is a library for efficient similarity search.\"\n",
    "]\n",
    "\n",
    "# Step 2: Embed corpus with SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast model\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# Step 3: Build FAISS index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# Step 4: Query input & embed it\n",
    "query = \"What is retrieval augmented generation?\"\n",
    "query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "# Step 5: Retrieve top-k similar documents (k=2)\n",
    "k = 2\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "retrieved_docs = [corpus[i] for i in indices[0]]\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}: {doc}\")\n",
    "\n",
    "# Step 6: Generate answer using BART conditioned on retrieved docs\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Combine retrieved docs as context + question\n",
    "input_text = \" \".join(retrieved_docs) + \" Question: \" + query\n",
    "inputs = tokenizer([input_text], max_length=1024, truncation=True, return_tensors='pt')\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    num_beams=4, \n",
    "    max_length=100, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6f6780-8fc2-4a51-bfcb-3c793c92b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved documents:\n",
      "1. Nvidia's CEO emphasized the importance of AI in future product development.\n",
      "2. Stock analysts predict Nvidia will benefit from increasing demand for AI hardware.\n",
      "3. Nvidia reported a 50% increase in revenue driven by growth in AI chips.\n",
      "\n",
      "Generated Answer:\n",
      " Nvidia reported a 50% increase in revenue driven by growth in AI chips. Nvidia's CEO emphasized the importance of AI in future product development.Stock analysts predict Nvidia will benefit from increasing demand for AI hardware. Use the following information to answer the question: What is Nvidia's strategy for AI growth?\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Sample Nvidia-related news snippets (replace with real data in practice)\n",
    "corpus = [\n",
    "    \"Nvidia reported a 50% increase in revenue driven by growth in AI chips.\",\n",
    "    \"The supply chain issues have caused delays in Nvidia's GPU shipments.\",\n",
    "    \"Nvidia's CEO emphasized the importance of AI in future product development.\",\n",
    "    \"Recent earnings call highlighted Nvidia's investment in autonomous vehicle technology.\",\n",
    "    \"Stock analysts predict Nvidia will benefit from increasing demand for AI hardware.\",\n",
    "    \"Nvidia faces regulatory scrutiny regarding its recent acquisitions.\",\n",
    "    \"The company expanded its data center business with new AI-focused GPUs.\",\n",
    "    \"Nvidia announced a partnership with leading cloud providers to enhance AI services.\"\n",
    "]\n",
    "\n",
    "# Initialize embedding model (small, fast model)\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed the corpus documents\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# Load BART model & tokenizer for generation\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "def rag_answer(query, top_k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # Retrieve top_k relevant documents\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [corpus[i] for i in indices[0]]\n",
    "\n",
    "    print(\"\\nRetrieved documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{i}. {doc}\")\n",
    "\n",
    "    # Prepare input for generation with retrieved docs + query prompt\n",
    "    input_text = (\n",
    "        \"Use the following information to answer the question:\\n\"\n",
    "        + \"\\n\".join(retrieved_docs)\n",
    "        + f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([input_text], max_length=1024, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate the answer\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        num_beams=4,\n",
    "        max_length=100,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Example query\n",
    "query = \"What is Nvidia's strategy for AI growth?\"\n",
    "\n",
    "answer = rag_answer(query)\n",
    "print(\"\\nGenerated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d75f2212-0c25-40a3-ae7e-d1ffe143a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_nvidia_news(api_key, max_articles=20):\n",
    "    url = (\"https://newsapi.org/v2/everything?\"\n",
    "           \"q=Nvidia&\"\n",
    "           \"sortBy=publishedAt&\"\n",
    "           f\"pageSize={max_articles}&\"\n",
    "           f\"apiKey={api_key}\")\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['status'] != 'ok':\n",
    "        raise Exception(\"NewsAPI error: \" + data.get('message', 'Unknown error'))\n",
    "\n",
    "    articles = data.get('articles', [])\n",
    "    corpus = []\n",
    "    for art in articles:\n",
    "        title = art.get('title', '')\n",
    "        desc = art.get('description', '')\n",
    "        content = art.get('content', '')\n",
    "        # Combine title + description + content (fallback to best available)\n",
    "        text = \" \".join([t for t in [title, desc, content] if t])\n",
    "        if text:\n",
    "            corpus.append(text)\n",
    "    return corpus\n",
    "\n",
    "# Usage example:\n",
    "# api_key = \"YOUR_NEWSAPI_KEY\"\n",
    "# nvidia_corpus = fetch_nvidia_news(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9616da26-82b4-4a08-a99d-6f6a14a0c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (3.5)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     --------------- ------------------------ 2.9/7.4 MB 28.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 32.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\senth\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2023.11.17)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\senth\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13566 sha256=ca271ff70bb2b7ba0b74775df94c1ecadafe92893eeb763a689b56719930f432\n",
      "  Stored in directory: c:\\users\\senth\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\f8\\cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3358 sha256=19dde774a6adb2976a13b58bb68e8050af3bbec0e413643c650a16e042327563\n",
      "  Stored in directory: c:\\users\\senth\\appdata\\local\\pip\\cache\\wheels\\80\\d5\\72\\9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398388 sha256=4b4b44d05b9d8722733b23b8f09e066653b7adc02c6f7427b08538ce1163b297\n",
      "  Stored in directory: c:\\users\\senth\\appdata\\local\\pip\\cache\\wheels\\3a\\a1\\46\\8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=80170e1823ae2b682b5f2fdc0f8a1ea67f63199abf0c0c522d6a6bbb86a3ae91\n",
      "  Stored in directory: c:\\users\\senth\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\n",
      "\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   ------------- -------------------------- 2/6 [jieba3k]\n",
      "   -------------------- ------------------- 3/6 [feedparser]\n",
      "   -------------------- ------------------- 3/6 [feedparser]\n",
      "   --------------------------------- ------ 5/6 [newspaper3k]\n",
      "   --------------------------------- ------ 5/6 [newspaper3k]\n",
      "   ---------------------------------------- 6/6 [newspaper3k]\n",
      "\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'tinysegmenter' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tinysegmenter'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'feedfinder2' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'feedfinder2'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'jieba3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'jieba3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69833db8-fd42-4513-aa50-0f12d7323e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 1:\n",
      "CNBC Investing Club: Video. CLICK TO READ THE DISCLAIMER. NO FIDUCIARY OBLIGATION OR DUTY EXISTS, OR IS CREATED, BY VIRTUE OF YOUR RECEIPT OF INVESTING CLUB INFORMATION. NO SPECIFIC OUTCOME OR PROFIT IS GUARANTEED....\n",
      "\n",
      "\n",
      "Article 2:\n",
      "Pro News and Analysis. ...\n",
      "\n",
      "\n",
      "Article 3:\n",
      "Wall Street's official 2025 stock market outlook: The latest CNBC Market Strategist Survey. Where do the sharpest minds on Wall Street believe the market is headed? The CNBC Pro exclusive Market Strategist Survey is a roundup of year-end targets for the S & P 500 from top Wall Street strategists, updated quarterly, or whenever there is a material change to the forecasts. Here are the current 2025 targets from top strategists: Maximum target: 6,700 - Brian Belski, BMO Minimum target: 5,600 â€”Julia...\n",
      "\n",
      "\n",
      "Article 4:\n",
      "CNBC Pro Stock Lists: Here are the latest stocks including all-weather plays. CNBC Pro Stock lists are tools for investors to find ideas fitting certain investing themes. We create the lists using screening techniques used by the pros, along with our access to some of the best research analysts and investors on Wall Street. These are not portfolios, but a starting point for research by investors with a particular macro or industry view. Check back as we add more lists...as well as more stocks to...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import newspaper\n",
    "\n",
    "def scrape_nvidia_news(url, max_articles=10):\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    corpus = []\n",
    "    for i, article in enumerate(paper.articles):\n",
    "        if i >= max_articles:\n",
    "            break\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.title + \". \" + article.text\n",
    "            corpus.append(text)\n",
    "        except:\n",
    "            continue\n",
    "    return corpus\n",
    "\n",
    "# Example: scrape Nvidia news from CNBC\n",
    "url = \"https://www.cnbc.com/nvidia/\"\n",
    "nvidia_news = scrape_nvidia_news(url, max_articles=5)\n",
    "\n",
    "for i, news in enumerate(nvidia_news, 1):\n",
    "    print(f\"\\nArticle {i}:\\n{news[:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7525c4-6fa9-40b5-aa57-6c328b68edae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting article links...\n",
      "Found 0 articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def get_article_links(url, max_links=5):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find headline links\n",
    "    anchors = soup.find_all('a', class_='LatestNews-headline', limit=max_links)\n",
    "    links = []\n",
    "    for a in anchors:\n",
    "        href = a.get('href')\n",
    "        if href and href.startswith(\"https://www.cnbc.com/\"):\n",
    "            links.append(href)\n",
    "    return links\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title + \"\\n\\n\" + article.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# CNBC Nvidia news page\n",
    "url = \"https://www.cnbc.com/nvidia/\"\n",
    "\n",
    "print(\"Getting article links...\")\n",
    "article_links = get_article_links(url)\n",
    "\n",
    "print(f\"Found {len(article_links)} articles.\\n\")\n",
    "\n",
    "for i, link in enumerate(article_links, 1):\n",
    "    print(f\"Article {i} URL: {link}\")\n",
    "    text = extract_article_text(link)\n",
    "    if text:\n",
    "        print(f\"Article {i} text preview:\\n{text[:500]}...\\n\")\n",
    "    else:\n",
    "        print(f\"Failed to extract Article {i}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ffcabf7-259c-43a3-878c-866553d515b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting article links...\n",
      "Found 0 articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def get_article_links_simple(url, max_links=5):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # Simple filter for CNBC Nvidia news article URLs:\n",
    "        if 'nvidia' in href and href.startswith('https://www.cnbc.com/') and href not in links:\n",
    "            links.append(href)\n",
    "        if len(links) >= max_links:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title + \"\\n\\n\" + article.text\n",
    "    except Exception as e:\n",
    "        return f\"Failed to extract article: {e}\"\n",
    "\n",
    "url = \"https://www.cnbc.com/nvidia/\"\n",
    "\n",
    "print(\"Getting article links...\")\n",
    "article_links = get_article_links_simple(url)\n",
    "\n",
    "print(f\"Found {len(article_links)} articles.\\n\")\n",
    "\n",
    "for i, link in enumerate(article_links, 1):\n",
    "    print(f\"Article {i} URL: {link}\")\n",
    "    text = extract_article_text(link)\n",
    "    print(f\"Article {i} text preview:\\n{text[:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83d024d0-fe8f-41d4-b7ed-7a6685923234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Reuters Nvidia news links...\n",
      "Found 0 articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def get_reuters_nvidia_links(url, max_links=5):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # Reuters Nvidia news URLs typically start with /technology/ or /business/\n",
    "        if href.startswith('/technology/') or href.startswith('/business/'):\n",
    "            full_url = 'https://www.reuters.com' + href\n",
    "            if 'nvidia' in full_url.lower() and full_url not in links:\n",
    "                links.append(full_url)\n",
    "        if len(links) >= max_links:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title + \"\\n\\n\" + article.text\n",
    "    except Exception as e:\n",
    "        return f\"Failed to extract article: {e}\"\n",
    "\n",
    "url = \"https://www.reuters.com/companies/NVDA.O/news\"\n",
    "\n",
    "print(\"Getting Reuters Nvidia news links...\")\n",
    "article_links = get_reuters_nvidia_links(url)\n",
    "\n",
    "print(f\"Found {len(article_links)} articles.\\n\")\n",
    "\n",
    "for i, link in enumerate(article_links, 1):\n",
    "    print(f\"Article {i} URL: {link}\")\n",
    "    text = extract_article_text(link)\n",
    "    print(f\"Article {i} text preview:\\n{text[:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb485510-5a75-4edf-b7d2-2eec0427ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\senth\\anaconda3\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\senth\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd2863b0-d85d-4e2c-87d1-aaf378acb51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Nvidia News from Google News RSS:\n",
      "\n",
      "1. Nvidia CEO: If I were a 20-year-old again today, this is the field I would focus on in college - CNBC\n",
      "Link: https://news.google.com/rss/articles/CBMi3AFBVV95cUxPcTNyQktVU0dqMlBLbmxKelQ4UEZKb2VrWU5Xay0zclBOei01Q0I3UWsyY0xyeEgxZ05ZZGdnMENMaFR0T3B2SnNsbERmLUVRUE82UW5wMTN0NmR6bFd6dExNS0R1ZHdEUVFhbmluZjItdUhqdTN3X19PMVNRQ05aQkVGSzc5QlVfOWtXMWlqN2VBcERWM3hJZ1A0dzBsTGNlZVVZZG5HNFlzcjMxMC1VVDFlSXlvb2w4S0VoRGw4SFZNckdOdGUwdkNFdWxkV3ZaWWZId0ktXzdYZHlH0gHiAUFVX3lxTE9RaDBHcWg3RUkxYmQ4M192RXdsb3BFT1dWWW5zb3U4RUFQaDJSaWRZd09meXF1ZTdXallNeFlaSmNOYllrdV9FdGJHM0xLUHZVUUJTd0lUMXVBVllDOTMzbmFkUUNmcGRIc2h5NDYtWjhqSzFJZkxwc1hLbDdZLWZwTTBiQnZHc1ZSNmwzYjdFTk1mSFNkSktKOERwdTV2QWZMUGFSMEl0di12NzRlTVk3THZwdWhuYy0xTmhhaTg1OFk1SFJ1NjJOQnRMSjdiVHZUZ08zRF9TUFNZR2ppWGhLTmc?oc=5\n",
      "\n",
      "2. Nvidia CEO Jensen Huang sells an additional $12.94 million worth of shares - CNBC\n",
      "Link: https://news.google.com/rss/articles/CBMifkFVX3lxTE9tYzRFMUpjak1ZMTR5czVvamhXWFFYMmRmZ1RERDBKZy1XOFhuNUplYmhZdHVhODJVeVd1UWE3cmM0VTRZZG5OTkVPSHU4TDFGVHlVR1pLeU9NM0tZM1ZIMF91bFVJTk81MzBHZ0JkRVAyVC1zWkNxMmxLYk8td9IBgwFBVV95cUxNTHFDQVpuNkhQYXp0c0dxT282SFRwNmhDR2h1ZDFyWWNhTEZUZ3EzTWRjYkczS3JNRWtTQWRtZnhXMFVEbU5uaUdDcUVYYXlKY25XSXA0S0tlcnEwd01SclZiNXA0U0ZOYXZoZGhCSDVUUFhITmpwdzFnMDBZYnc4Qm51TQ?oc=5\n",
      "\n",
      "3. Nvidia's China restart faces production obstacles, The Information reports - Reuters\n",
      "Link: https://news.google.com/rss/articles/CBMitwFBVV95cUxPbF9QOGhXY1V5NnEtbVI3cjZjYi1tQkF1enJMUHJNbVNWaW94TWNQVlhoWjczcFR4d2JiZGt1WVdzUDNYY2ZVYlFCdV85OUd0aTJScnpfdkJ3NjBTWDM4UjFzQVZxMVhQd3ItMWl4SVkzOWhycGpoVkVuQkpnZlR0VmttejdOZ20xdVAwRmQydlBMSjE3azFZZDhza1JlX1BrY0RSOEp4X2pVOXdfaFRTZGUtdFo2RUk?oc=5\n",
      "\n",
      "4. Nvidia and other stocks just became the most overbought names on Wall Street this week - CNBC\n",
      "Link: https://news.google.com/rss/articles/CBMipgFBVV95cUxNcW1QWXhuOGQwbjBucTR5OHg5eklpVF9RYUN1NEcydlZJZkJVaURuSjdGczhWS0FXZzhncnZ0VjgxTXZ1SkY3OUtFUmtkRXJER24zc3hGQWVmUFB1ZVRkaUZoX19NWV84NXpMYmFqbGszRUJYZHhsdFkzaFllQlhkSUhKeXlkRVg0R08wcDBQckswSmdZQ3BFOE9IOHJ2QVJuTlp2TkxB?oc=5\n",
      "\n",
      "5. Opinion | Heard of Nvidia? Hereâ€™s why you should pay attention. - The Washington Post\n",
      "Link: https://news.google.com/rss/articles/CBMihgFBVV95cUxPMlJRa3dlcl95dzZieWZQcTNFY0NLTVNJSGRvdUlxWVM4eC1qM1RldHJLclY0aHo5a1hJVmJ0VzlnZlpieTYzbkt3R3ZBT2Z5VUtVM2hjZWJzVUlDbVFtODFMblZlSV9kMk94bktGRHlHQUl3UFdubkt5cVpjQVVPV0hZUzdCdw?oc=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "def get_google_news_rss(query, max_articles=5):\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_articles]:\n",
    "        title = entry.title\n",
    "        link = entry.link\n",
    "        articles.append((title, link))\n",
    "    return articles\n",
    "\n",
    "query = \"Nvidia\"\n",
    "\n",
    "news = get_google_news_rss(query)\n",
    "\n",
    "print(\"Latest Nvidia News from Google News RSS:\\n\")\n",
    "for i, (title, link) in enumerate(news, 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "    print(f\"Link: {link}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93808461-d579-4af7-b02b-ca99b8e4443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
